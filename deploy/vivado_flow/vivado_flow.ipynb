{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vivado Flow\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "The current set of notebooks are under constant development.\n",
    "\n",
    "### Update Tutorial Repository\n",
    "\n",
    "If you have previously cloned the tutorial repository, you may need to get the latest versions of the notebooks.\n",
    "\n",
    "First check the status of your repository:\n",
    "```\n",
    "cd hls4ml-tutorial\n",
    "make clean\n",
    "git status \n",
    "```\n",
    "\n",
    "You may have some _modified_ notebooks. For example:\n",
    "\n",
    "```\n",
    "# On branch csee-e6868-spring2021\n",
    "# Changes not staged for commit:\n",
    "#   (use \"git add <file>...\" to update what will be committed)\n",
    "#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
    "#\n",
    "#\tmodified:   part1_getting_started.ipynb\n",
    "#\tmodified:   part2_advanced_config.ipynb\n",
    "#\tmodified:   part2b_advanced_config.ipynb\n",
    "#\n",
    "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
    "```\n",
    "\n",
    "You can make a copy of those modified notebooks if you had significat changes, otherwise the easiest thing to do is to discard those changes.\n",
    "\n",
    "**ATTENTION** You will loose your local changes!\n",
    "\n",
    "```\n",
    "git checkout *.ipynb\n",
    "```\n",
    "\n",
    "At this point, you can update you copy of the repository:\n",
    "```\n",
    "git pull\n",
    "```\n",
    "\n",
    "\n",
    "### Update Conda Environment\n",
    "\n",
    "It is likely that you are running this notebook in the Conda environment `hls4ml-tutorial-cu`.\n",
    "\n",
    "If you did not do that yet, you should update the `hls4ml` packages with the latest changes in the working branch.\n",
    "\n",
    "```\n",
    "conda activate hls4ml-tutorial-cu\n",
    "pip uninstall hls4ml\n",
    "pip install git+https://github.com/GiuseppeDiGuglielmo/hls4ml.git@gdg/cosmetics#egg=hls4ml[profiling]\n",
    "```\n",
    "\n",
    "You may need to restart the Jupyter notebook.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We're going to train a fully connected neural network with QKeras on the jet tagging dataset and run it baremetal on Zynq-class boards (ZCU106, Ultra96, Pynq-Z1, MiniZed).\n",
    "\n",
    "This is an overview of the flow. We reference some of steps in this notebook.\n",
    "\n",
    "![vivado-flow](doc/vivado_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Choose the target board. For the time being, you can use `minized`, `pynqz1`, `pynqz2`, `cmoda735t`. You may need to install the proper board files for the chosen board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZCU106\n",
    "#board_name='zcu106'\n",
    "#fpga_part='xczu7ev-ffvc1156-2-e'\n",
    " \n",
    "## Ultra96\n",
    "#board_name='ultra96'\n",
    "#fpga_part='xczu3eg-sbva484-1-e'\n",
    "\n",
    "## Pynq-Z1\n",
    "#board_name='pynqz1'\n",
    "#fpga_part='xc7z020clg400-1'\n",
    "\n",
    "## Pynq-Z2\n",
    "board_name='pynqz2'\n",
    "fpga_part='xc7z020clg400-1'\n",
    "\n",
    "## MiniZed\n",
    "#board_name='minized'\n",
    "#fpga_part='xc7z007sclg225-1'\n",
    "\n",
    "##Cmod A7-35t\n",
    "#board_name='cmoda735t'\n",
    "#fpga_part='xc7a35tcpg236-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries, call the magic functions, and setup the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Xilinx Vivado HLS is in the PATH\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import hls4ml\n",
    "\n",
    "from callbacks import all_callbacks\n",
    "import plotting\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vivado/2019.1/bin:' + os.environ['PATH']\n",
    "\n",
    "def is_tool(name):\n",
    "    from distutils.spawn import find_executable\n",
    "    return find_executable(name) is not None\n",
    "\n",
    "print('-----------------------------------')\n",
    "if not is_tool('vivado_hls'):\n",
    "    print('Xilinx Vivado HLS is NOT in the PATH')\n",
    "else:\n",
    "    print('Xilinx Vivado HLS is in the PATH')\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "This is a lot like the previous notebooks, so we will go through quickly.\n",
    "\n",
    "First, we fetch the dataset from file, do the normalization and make a train and test split.\n",
    "\n",
    "We save the test dataset to files so that we can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load processed test data\n",
    "from sklearn.utils import shuffle\n",
    "X = np.load('./test_data/test_data.npy', allow_pickle=True)\n",
    "y = np.load('./test_data/test_data_ground_truths.npy', allow_pickle=True)\n",
    "y_keras = []\n",
    "#use a quarter of the test_set to save time\n",
    "for i in range(len(X)):\n",
    "    quarter = int(len(X[i])/4)\n",
    "    assert len(X) == len(y)\n",
    "    X[i], y[i] = shuffle(X[i], y[i])\n",
    "    X[i], y[i] = X[i][0:quarter],  y[i][0:quarter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 640)]             0         \n",
      "_________________________________________________________________\n",
      "q_dense (QDense)             (None, 128)               82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation (QActivation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_1 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_1 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_2 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_2 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_3 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_3 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_4 (QDense)           (None, 8)                 1032      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "q_activation_4 (QActivation) (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "q_dense_5 (QDense)           (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_5 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_6 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_6 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_7 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_7 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_8 (QDense)           (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "q_activation_8 (QActivation) (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "q_dense_9 (QDense)           (None, 640)               82560     \n",
      "=================================================================\n",
      "Total params: 269,992\n",
      "Trainable params: 267,928\n",
      "Non-trainable params: 2,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras_model\n",
    "train = False\n",
    "#not os.path.exists('model/KERAS_check_best_model.h5')\n",
    "if train:\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        \n",
    "    print(\"Shape of training data element is: {}\".format(train_data[0].shape))\n",
    "    history = model.fit(train_data,\n",
    "                        train_data,\n",
    "                        epochs=100,\n",
    "                        batch_size=512,\n",
    "                        shuffle=true,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "\n",
    "else:\n",
    "    model_file = \"{model}/model_{machine_type}.hdf5\".format(model=\"./model/train_config_bits_6_frames_5_mels_128_encDims_8_bn_True_l1reg_0_expPower_3_beginSpar_0_finSpar_0.8\",\n",
    "                                                              machine_type=\"ToyCar\")\n",
    "    #model_file = \"model/KERAS_check_best_model.hdf5\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(\"{} model not found at path \".format(model_file))\n",
    "    model = keras_model.load_model(model_file)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy\n",
    "\n",
    "Do not expect a good accuracy because of the low amount of neurons. I could have done better than this, but as long as it fits both Pynq-Z1 and MiniZed, it is fine with us."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotting\n",
    "import numpy\n",
    "\n",
    "#load processed test data\n",
    "X = np.load('./test_data/test_data.npy', allow_pickle=True)\n",
    "y = np.load('./test_data/test_data_ground_truths.npy', allow_pickle=True)\n",
    "y_keras = []\n",
    "#use a quarter of the test_set to save time\n",
    "for i in range(len(X)):\n",
    "    quarter = int(len(X[i])/4)\n",
    "    assert len(X) == len(y)\n",
    "    X[i], y[i] = shuffle(X[i], y[i])\n",
    "    X[i], y[i] = X[i][0:quarter],  y[i][0:quarter]\n",
    "\n",
    "#perform inference\n",
    "for index, X_data in enumerate(X):\n",
    "    y_pred = [0. for ind in X_data]\n",
    "    for file_idx, X_test in enumerate(X_data):\n",
    "        predictions = model.predict(X_test)\n",
    "        errors = np.mean(np.square(X_test-predictions), axis=1)\n",
    "        y_pred[file_idx] = numpy.mean(errors)\n",
    "        \n",
    "    #generate auc and roc metrics\n",
    "    y_test = y[index]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    y_keras.append(y_pred)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = 'AUC m_{} = {}'.format(index, round(roc_auc,2)), linewidth = 1.5)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--', linewidth=1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an hls4ml configuration (Step 2)\n",
    "\n",
    "Notice we're using `Strategy: Resource` for every layer, and `ReuseFactor: 64`. The Programmable Logic (FPGA part) of the Pynq-Z1 SoC is not big compared to VU9P type of parts.\n",
    "\n",
    "We also use some settings which are good for QKeras.\n",
    "\n",
    "Notice the `fpga_part:'xc7z020clg400-1'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Model\n",
      "  ReuseFactor:       128\n",
      "  Strategy:          Resource\n",
      "  Precision:         ap_fixed<7,4>\n",
      "LayerName\n",
      "  input_1\n",
      "    Precision:       ap_fixed<7,7>\n",
      "  q_dense\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_1\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_1\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_1\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_2\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_2\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_2\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_3\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_3\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_3\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_4\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_4\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_4\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_5\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_5\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_5\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_6\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_6\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_6\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_7\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_7\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_7\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_8\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "  batch_normalization_8\n",
      "    Precision\n",
      "      scale:         ap_fixed<7,4>\n",
      "      bias:          ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_activation_8\n",
      "    Precision\n",
      "      result:        ap_fixed<7,4>\n",
      "    ReuseFactor:     128\n",
      "  q_dense_9\n",
      "    Precision\n",
      "      weight:        ap_fixed<7,1>\n",
      "      bias:          ap_fixed<7,1>\n",
      "    ReuseFactor:     128\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "hls_config['Model'] = {}\n",
    "hls_config['Model']['ReuseFactor'] = 128\n",
    "hls_config['Model']['Strategy'] = 'Resource'\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<7,4>'\n",
    "hls_config['LayerName']['input_1']['Precision'] = 'ap_fixed<7,7>'\n",
    "\n",
    "hls_config['LayerName']['q_dense']['Precision']['weight'] = 'ap_fixed<7,1>'\n",
    "hls_config['LayerName']['q_dense']['Precision']['bias'] = 'ap_fixed<7,1>'\n",
    "hls_config['LayerName']['q_dense']['ReuseFactor'] = 128\n",
    "\n",
    "hls_config['LayerName']['batch_normalization']['Precision']['scale'] = 'ap_fixed<7,4>'\n",
    "hls_config['LayerName']['batch_normalization']['Precision']['bias'] = 'ap_fixed<7,4>'\n",
    "hls_config['LayerName']['batch_normalization']['ReuseFactor'] = 128\n",
    "\n",
    "hls_config['LayerName']['q_activation']['Precision']['result'] = 'ap_fixed<7,4>'\n",
    "hls_config['LayerName']['q_activation']['ReuseFactor'] = 128\n",
    "\n",
    "for i in range(1,9):\n",
    "    \n",
    "    hls_config['LayerName']['q_dense_{}'.format(i)]['Precision']['weight'] = 'ap_fixed<7,1>'\n",
    "    hls_config['LayerName']['q_dense_{}'.format(i)]['Precision']['bias'] = 'ap_fixed<7,1>'\n",
    "    hls_config['LayerName']['q_dense_{}'.format(i)]['ReuseFactor'] = 128\n",
    "\n",
    "    hls_config['LayerName']['batch_normalization_{}'.format(i)]['Precision']['scale'] = 'ap_fixed<7,4>'\n",
    "    hls_config['LayerName']['batch_normalization_{}'.format(i)]['Precision']['bias'] = 'ap_fixed<7,4>'\n",
    "    hls_config['LayerName']['batch_normalization_{}'.format(i)]['ReuseFactor'] = 128\n",
    "\n",
    "    hls_config['LayerName']['q_activation_{}'.format(i)]['Precision']['result'] = 'ap_fixed<7,4>'\n",
    "    hls_config['LayerName']['q_activation_{}'.format(i)]['ReuseFactor'] = 128\n",
    "    \n",
    "#final output\n",
    "hls_config['LayerName']['q_dense_9']['Precision']['weight'] = 'ap_fixed<7,1>'\n",
    "hls_config['LayerName']['q_dense_9']['Precision']['bias'] = 'ap_fixed<7,1>'\n",
    "hls_config['LayerName']['q_dense_9']['ReuseFactor'] = 128\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(hls_config)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and Compile\n",
    "\n",
    "You can set some target specific configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the `interface`, which for our current setup should always be `m_axi`.\n",
    "- Define the  width of the AXI bus. For the time being, use `16` that is each clock cycle you transfer a single input or output value (`ap_fixed<16,*>`).\n",
    "- Define the implementation. For the time being, use `serial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = 'm_axi' # 's_axilite', 'm_axi', 'hls_stream'\n",
    "axi_width = 16 # 16, 32, 64\n",
    "implementation = 'serial' # 'serial', 'dataflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='hls/' + board_name + '_' + interface + '_' + str(axi_width) + '_' + implementation + '_prj' \n",
    "\n",
    "backend_config = hls4ml.converters.create_backend_config(fpga_part=fpga_part)\n",
    "backend_config['ProjectName'] = 'jet_tagger'\n",
    "backend_config['KerasModel'] = model\n",
    "backend_config['HLSConfig'] = hls_config\n",
    "backend_config['OutputDir'] = output_dir\n",
    "backend_config['Backend'] = 'Pynq'\n",
    "backend_config['Interface'] = interface\n",
    "backend_config['IOType'] = 'io_parallel'\n",
    "backend_config['AxiWidth'] = str(axi_width)\n",
    "backend_config['Implementation'] = implementation\n",
    "backend_config['ClockPeriod'] = 10\n",
    "\n",
    "#print(\"-----------------------------------\")\n",
    "#plotting.print_dict(backend_config)\n",
    "#print(\"-----------------------------------\")\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(backend_config)\n",
    "\n",
    "_ = hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutputDir:           hls/pynqz2_m_axi_16_serial_prj\n",
      "ProjectName:         jet_tagger\n",
      "XilinxPart:          xc7z020clg400-1\n",
      "ClockPeriod:         10\n",
      "Backend:             Pynq\n",
      "IOType:              io_parallel\n",
      "HLSConfig\n",
      "  Model\n",
      "    ReuseFactor:     128\n",
      "    Strategy:        Resource\n",
      "    Precision:       ap_fixed<7,4>\n",
      "  LayerName\n",
      "    input_1\n",
      "      Precision:     ap_fixed<7,7>\n",
      "    q_dense\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_1\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_1\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_1\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_2\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_2\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_2\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_3\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_3\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_3\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_4\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_4\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_4\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_5\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_5\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_5\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_6\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_6\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_6\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_7\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_7\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_7\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_8\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "    batch_normalization_8\n",
      "      Precision\n",
      "        scale:       ap_fixed<7,4>\n",
      "        bias:        ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_activation_8\n",
      "      Precision\n",
      "        result:      ap_fixed<7,4>\n",
      "      ReuseFactor:   128\n",
      "    q_dense_9\n",
      "      Precision\n",
      "        weight:      ap_fixed<7,1>\n",
      "        bias:        ap_fixed<7,1>\n",
      "      ReuseFactor:   128\n",
      "KerasModel:          <tensorflow.python.keras.engine.functional.Functional object at 0x7f95c27a4610>\n",
      "Interface:           m_axi\n",
      "AxiWidth:            16\n",
      "Implementation:      serial\n",
      "Stamp:               C6Ef8dd4\n"
     ]
    }
   ],
   "source": [
    "plotting.print_dict(backend_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False,synth=True,export=True)\n",
    "\n",
    "hls4ml.report.read_vivado_report(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Reference\n",
    "\n",
    "See the resources availables on different boards.\n",
    "\n",
    "```\n",
    "+-----------------+---------+-------+--------+-------+-----+                    \n",
    "|                 |               Resource                 |\n",
    "+-----------------+---------+-------+--------+-------+-----+\n",
    "|      Board      | BRAM_18K| DSP48E|   FF   |  LUT  | URAM|\n",
    "+-----------------+---------+-------+--------+-------+-----+\n",
    "|   PYNQ-Z1/Z2    |      280|    220|  106400|  53200|    0|\n",
    "+-----------------+---------+-------+--------+-------+-----+\n",
    "|     MiniZed     |      100|     66|   28800|  14400|    0|\n",
    "+-----------------+---------+-------+--------+-------+-----+\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .dat Files (Step 3)\n",
    "\n",
    "The .dat files are used\n",
    "- during the following `csim` step\n",
    "- to generate the header files for SDK"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = open(output_dir + '/tb_data/tb_input_features.dat', 'w')\n",
    "for i in range(X_test.shape[0]):\n",
    "    for j in range(X_test.shape[1]):\n",
    "        f.write('{} '.format(X_test[i][j]))\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open(output_dir + '/tb_data/tb_output_predictions.dat', 'w')\n",
    "for i in range(y_test.shape[0]):\n",
    "    for j in range(y_test.shape[1]):\n",
    "        f.write('{} '.format(y_test[i][j]))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Vivado HLS csim (Step 4)\n",
    "\n",
    "At this step we generate simulation traces out from the hls4ml-model.\n",
    "\n",
    "Run the following cell to run Vivado HLS GUI:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!cd $output_dir && vivado_hls -p jet_tagger_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** Click the button to `Run C Simulation`.\n",
    "\n",
    "This will generate simulation traces with fixed-point arythmetic.\n",
    "\n",
    "When completed close Vivado HLS GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate IP in a Vivado Project and Generate Bitstream (Step 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!cd sys/$board_name && make clean sys-gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Tell the user how to visualize the `Block Diagram` to get a better understanding of the IP integration with both Zynq and MicroBlaze PS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Software in Vivado SDK and Run HW/SW on the Board (Step 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Vivado SDK project.\n",
    "\n",
    "- `make sdk` to configure an application with register polling\n",
    "- `make sdk-irq` to configure an application with interrupts (default)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!source /tools/Xilinx/Vivado/2019.1/settings64.sh && cd sdk/$board_name && make clean sdk-irq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!xterm -e \"sleep 1 && source /tools/Xilinx/Vivado/2019.1/settings64.sh && cd sdk/$board_name && make gui && sleep infinity\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open a serial console, for example\n",
    "```\n",
    "sudo minicom -D /dev/ttyUSB0\n",
    "```\n",
    "and see \n",
    "\n",
    "![serial-console](doc/serial_console.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml-tutorial-cu",
   "language": "python",
   "name": "hls4ml-tutorial-cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
